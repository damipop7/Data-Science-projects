{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a589c0-f343-4845-84fc-7f0ad506554c",
   "metadata": {},
   "source": [
    "# Image Classification with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e446c96-c46f-4a2a-bdac-ec8fbf66974e",
   "metadata": {},
   "source": [
    "##### Classification is the task of categorizing the noun classes based on their features.\n",
    "\n",
    "##### In most classification problems, machine learning algorithms will do the job.\n",
    "\n",
    "##### But while classifying a large dataset of images, you will need to use a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec87e39b-c5c0-4196-99af-45408e1e1151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting jax>=0.3.15 (from tensorflow)\n",
      "  Downloading jax-0.4.28-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.48.2)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow) (1.11.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading jax-0.4.28-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gast, jax, tensorboard\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "Successfully installed gast-0.4.0 jax-0.4.28 tensorboard-2.12.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c7da76-54b2-4d8a-981e-37ff82c6de28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fashion = keras.datasets.fashion_mnist\n",
    "(xtrain, ytrain), (xtest,ytest) = fashion.load_data() #xtrain, ytrain for training, xtest, ytest for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39ab318-4178-4bc9-aa7e-d8b50fd063cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16770a410>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgIndex= 9\n",
    "image= xtrain[imgIndex]\n",
    "print(\"Image label: \", ytrain[imgIndex])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7402e1-983a-491c-a751-5e86d4badd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "881c5cd2-51b1-4392-a496-1152033e5882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation= \"relu\"),\n",
    "    keras.layers.Dense(100, activation= \"relu\"),\n",
    "    keras.layers.Dense(10, activation= \"softmax\")\n",
    "])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "354bcc3e-4709-43b7-a817-0a6df9310965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the training data into training nad validation sets \n",
    "xvalid, xtrain = xtrain[:5000]/255.0, xtrain[5000:]/255.0\n",
    "yvalid, ytrain = ytrain[:5000], ytrain[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeb8a10d-61f6-4c17-8237-f358ec1eeb13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 16ms/step - accuracy: 0.6845 - loss: 0.9905 - val_accuracy: 0.7992 - val_loss: 0.5562\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - accuracy: 0.8271 - loss: 0.5004 - val_accuracy: 0.8512 - val_loss: 0.4371\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 15ms/step - accuracy: 0.8422 - loss: 0.4480 - val_accuracy: 0.8604 - val_loss: 0.4106\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 16ms/step - accuracy: 0.8556 - loss: 0.4172 - val_accuracy: 0.8710 - val_loss: 0.3939\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.8611 - loss: 0.3948 - val_accuracy: 0.8696 - val_loss: 0.3830\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.8665 - loss: 0.3803 - val_accuracy: 0.8542 - val_loss: 0.3928\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.8693 - loss: 0.3711 - val_accuracy: 0.8774 - val_loss: 0.3610\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.3514 - val_accuracy: 0.8504 - val_loss: 0.4038\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8775 - loss: 0.3392 - val_accuracy: 0.8786 - val_loss: 0.3393\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8816 - loss: 0.3308 - val_accuracy: 0.8782 - val_loss: 0.3364\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8864 - loss: 0.3198 - val_accuracy: 0.8798 - val_loss: 0.3401\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.8870 - loss: 0.3131 - val_accuracy: 0.8802 - val_loss: 0.3362\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8906 - loss: 0.3017 - val_accuracy: 0.8832 - val_loss: 0.3345\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - accuracy: 0.8911 - loss: 0.3042 - val_accuracy: 0.8868 - val_loss: 0.3209\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.8940 - loss: 0.2936 - val_accuracy: 0.8892 - val_loss: 0.3139\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8973 - loss: 0.2852 - val_accuracy: 0.8878 - val_loss: 0.3164\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8987 - loss: 0.2756 - val_accuracy: 0.8836 - val_loss: 0.3222\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9005 - loss: 0.2787 - val_accuracy: 0.8896 - val_loss: 0.3099\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9024 - loss: 0.2690 - val_accuracy: 0.8898 - val_loss: 0.3128\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9051 - loss: 0.2617 - val_accuracy: 0.8860 - val_loss: 0.3163\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.9069 - loss: 0.2575 - val_accuracy: 0.8880 - val_loss: 0.3072\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9036 - loss: 0.2620 - val_accuracy: 0.8882 - val_loss: 0.3066\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9088 - loss: 0.2501 - val_accuracy: 0.8896 - val_loss: 0.3124\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9101 - loss: 0.2495 - val_accuracy: 0.8892 - val_loss: 0.3066\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9107 - loss: 0.2463 - val_accuracy: 0.8950 - val_loss: 0.2970\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9134 - loss: 0.2435 - val_accuracy: 0.8882 - val_loss: 0.2959\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.9155 - loss: 0.2362 - val_accuracy: 0.8964 - val_loss: 0.2898\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9198 - loss: 0.2292 - val_accuracy: 0.8938 - val_loss: 0.2917\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9209 - loss: 0.2270 - val_accuracy: 0.8908 - val_loss: 0.3046\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9195 - loss: 0.2250 - val_accuracy: 0.8942 - val_loss: 0.2911\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss= \"sparse_categorical_crossentropy\",\n",
    "             optimizer= \"sgd\",\n",
    "             metrics=[\"accuracy\"])\n",
    "history= model.fit(xtrain, ytrain, epochs= 30,\n",
    "                  validation_data=(xvalid,yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a49db92-1795-415e-8504-c25cb379acca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "new= xtest\n",
    "predictions = model.predict(new)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2236c0b4-8ce9-473b-8a1c-5fcfed6dd77c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 ... 8 1 5]\n"
     ]
    }
   ],
   "source": [
    "#train a classification network using neural network\n",
    "classes = np.argmax(predictions, axis= 1)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b79a727c-10b4-4bf6-aa40-1b95e365551e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.852\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of Image Classification using Neural Network\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract actual labels from the test dataset\n",
    "actual_labels = ytest  # Assuming you want to evaluate predictions for the first 5 samples\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, classes)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19214d-efdd-4b4b-99e3-7b42ed3f7c5e",
   "metadata": {},
   "source": [
    "With an accuracy of 85.2%, our neural network model, trained on the Fashion \n",
    "MNIST dataset using TensorFlow and Keras, showcases its ability to accurately \n",
    "classify clothing items based on their images, demonstrating the effectiveness \n",
    "of our model architecture and training approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7af2f4f-6ba8-4388-bb82-6195e204794d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8622763309269296\n",
      "Recall: 0.852\n",
      "F1 Score: 0.8502749674862358\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall, and F1 score of Image Classification using Neural Network\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(actual_labels, classes, average='weighted')\n",
    "recall = recall_score(actual_labels, classes, average='weighted')\n",
    "f1 = f1_score(actual_labels, classes, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfd37916-854f-4214-88f3-3d15b6256192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.23.5)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c136cb14-e676-4621-91c8-74a9bc5bda37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<1, -1, -1>, cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<1, -1, -1>, VDcn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Convert the grayscale image to RGB format\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m img_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_GRAY2RGB)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get the feature maps of the last conv layer and the model's prediction\u001b[39;00m\n\u001b[1;32m     30\u001b[0m feature_maps, prediction \u001b[38;5;241m=\u001b[39m cam_model\u001b[38;5;241m.\u001b[39mpredict(img_rgb), model\u001b[38;5;241m.\u001b[39mpredict(img_rgb)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<1, -1, -1>, cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<1, -1, -1>, VDcn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model (assuming it's already trained and saved)\n",
    "model = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "# Get the output of the last convolutional layer\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "\n",
    "# Create a new model that outputs the feature maps of the last conv layer\n",
    "cam_model = keras.Model(inputs=model.inputs, outputs=last_conv_layer.output)\n",
    "\n",
    "# Get the weights of the output layer (Dense layer) of the original model\n",
    "dense_weights = model.get_layer('predictions').get_weights()[0]\n",
    "\n",
    "# Load an example image from your dataset (e.g., xtrain[imgIndex])\n",
    "img = xtrain[imgIndex]\n",
    "\n",
    "# Preprocess the image (resize to the input size of the model and apply normalization)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = keras.applications.vgg16.preprocess_input(img)\n",
    "\n",
    "# Expand the dimensions of the image to match the model's input shape\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# Convert the grayscale image to RGB format\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Get the feature maps of the last conv layer and the model's prediction\n",
    "feature_maps, prediction = cam_model.predict(img_rgb), model.predict(img_rgb)\n",
    "\n",
    "# Calculate the class activation map\n",
    "cam = np.dot(feature_maps, dense_weights[:, prediction.argmax()])\n",
    "\n",
    "# Resize the CAM to match the size of the original image\n",
    "cam = cv2.resize(cam[0], (img.shape[2], img.shape[1]))\n",
    "\n",
    "# Normalize the CAM\n",
    "cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "\n",
    "# Apply colormap to visualize the CAM\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "\n",
    "# Overlay the CAM on the original image\n",
    "cam_image = cv2.addWeighted(cv2.cvtColor(img[0], cv2.COLOR_RGB2BGR), 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "# Visualize the original image and the CAM\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img[0])\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cam_image)\n",
    "plt.title('Class Activation Map (CAM)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454538a2-20fc-4ecf-827a-69d5aee4bf35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
